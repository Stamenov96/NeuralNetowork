import java.math.BigDecimal;
import java.math.RoundingMode;

  class BackPropagationProgram
  {
	  public static void main(String[] args)
    {
      try
      {
        System.out.println("\nBegin Neural Network Back-Propagation demo\n");

        System.out.println("Creating a 3-input, 4-hidden, 2-output neural network");
        System.out.println("Using sigmoid function for input-to-hidden activation");
        System.out.println("Using tanh function for hidden-to-output activation");
        NeuralNetwork nn = new NeuralNetwork(3, 4, 2);

        // arbitrary weights and biases
        double[] weights = new double[] {
          0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2,
          -2.0, -6.0, -1.0, -7.0,
          1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0,
          -2.5, -5.0 };

        System.out.println("\nInitial 26 random weights and biases are:");
        Helpers.ShowVector(weights);

        System.out.println("Loading neural network weights and biases");
        nn.SetWeights(weights);

        System.out.println("\nSetting inputs:");
        double[] xValues = new double[] { 1.0, 3.0, 3.0 };
        Helpers.ShowVector(xValues);

        double[] initialOutputs = nn.ComputeOutputs(xValues);
        System.out.println("Initial outputs:");
        Helpers.ShowVector(initialOutputs);

        double[] tValues = new double[] { 0.0, 1 }; // target (desired) values. note these only make sense for tanh output activation
        System.out.println("Target outputs to learn are:");
        Helpers.ShowVector(tValues);

        double eta = 0.90;  // learning rate - controls the maginitude of the increase in the change in weights. found by trial and error.
        double alpha = 0.04; // momentum - to discourage oscillation. found by trial and error.
        System.out.println("Setting learning rate (eta) = " + eta + " and momentum (alpha) = " + alpha);
        
        System.out.println("\nEntering main back-propagation compute-update cycle");
        System.out.println("Stopping when sum absolute error <= 0.01 or 1,000 iterations\n");
        int ctr = 0;
        double[] yValues = nn.ComputeOutputs(xValues); // prime the back-propagation loop
        System.out.println("output");
        Helpers.ShowVector(yValues);
        double error = Error(tValues, yValues);
//        while (ctr < 1000 && error > 0.01)
//        {
//          System.out.println("===================================================");
//          System.out.println("iteration = " + ctr);
//          System.out.println("Updating weights and biases using back-propagation");
//          nn.UpdateWeights(tValues, eta, alpha);
//          System.out.println("Computing new outputs:");
//          yValues = nn.ComputeOutputs(xValues);
//          Helpers.ShowVector(yValues);
//          System.out.println("\nComputing new error");
//          error = Error(tValues, yValues);
//          System.out.println("Error = " + error);
//          ++ctr;
//        }
//        System.out.println("===================================================");
//        System.out.println("\nBest weights and biases found:");
//        double[] bestWeights = nn.GetWeights();
//        Helpers.ShowVector(bestWeights);
//        
//        System.out.println("End Neural Network Back-Propagation demo\n");
       // Console.ReadLine();
      }
      catch (Exception ex)
      {
        System.out.println("Fatal: " + ex);
      //  Console.ReadLine();
      }
    } // Main

    static double Error(double[] target, double[] output) // sum absolute error. could put into NeuralNetwork class.
    {
      double sum = 0.0;
      for (int i = 0; i < target.length; ++i)
        sum += Math.abs(target[i] - output[i]);
      return sum;
    }

  } // class BackPropagation

  class NeuralNetwork
  {
    private int numInput;
    private int numHidden;
    private int numOutput;

    private double[] inputs;
    private double[][] ihWeights; // input-to-hidden
    private double[] ihSums;
    private double[] ihBiases;
    private double[] ihOutputs;

    private double[][] hoWeights;  // hidden-to-output
    private double[] hoSums;
    private double[] hoBiases;
    private double[] outputs;

    private double[] oGrads; // output gradients for back-propagation
    private double[] hGrads; // hidden gradients for back-propagation

    private double[][] ihPrevWeightsDelta;  // for momentum with back-propagation
    private double[] ihPrevBiasesDelta;

    private double[][] hoPrevWeightsDelta;
    private double[] hoPrevBiasesDelta;

    public NeuralNetwork(int numInput, int numHidden, int numOutput)
    {
      this.numInput = numInput;
      this.numHidden = numHidden;
      this.numOutput = numOutput;

      inputs = new double[numInput];
      ihWeights = Helpers.MakeMatrix(numInput, numHidden);
      ihSums = new double[numHidden];
      ihBiases = new double[numHidden];
      ihOutputs = new double[numHidden];
      hoWeights = Helpers.MakeMatrix(numHidden, numOutput);
      hoSums = new double[numOutput];
      hoBiases = new double[numOutput];
      outputs = new double[numOutput];

      oGrads = new double[numOutput];
      hGrads = new double[numHidden];

      ihPrevWeightsDelta = Helpers.MakeMatrix(numInput, numHidden);
      ihPrevBiasesDelta = new double[numHidden];
      
      hoPrevWeightsDelta = Helpers.MakeMatrix(numHidden, numOutput);
      hoPrevBiasesDelta = new double[numOutput];
    }

    
    
	public void step1(double[] tValues){
		   
        // 1. compute output gradients
        for (int i = 0; i < oGrads.length; ++i)
        {
        	//double derivative = ((new BigDecimal(1).subtract(ihOutputs[i])).multiply(ihOutputs[i])).doubleValue();//derivative of softmax
        	double derivative = (1-ihOutputs[i])*ihOutputs[i];
            oGrads[i]=derivative*(tValues[i]-outputs[i]);
        }
}

public void step2(){
// 2. compute hidden gradients
        for (int i = 0; i < hGrads.length; ++i)
        {            	
       // double derivative = ((new BigDecimal(1).subtract(ihOutputs[i])).multiply(ihOutputs[i])).doubleValue();
        double derivative = (1-ihOutputs[i])*ihOutputs[i];
        double sum = 0.0;
        for (int j = 0; j < numOutput; j++) {
        	sum+=oGrads[j]*hoWeights[i][j];
        }
        	hGrads[i]=derivative*(sum);
		
        }
//    System.out.println("BIG DEC");
//    System.out.println(sum);
//    System.out.println("Double");
//    System.out.println(sum2);
//     
     
  //  hGrads[i] = derivative.multiply(sum);
    //hGrads2[i] = derivative.doubleValue()*sum2;
    //hGrads[i]= new BigDecimal(hGrads2[i]);
//    System.out.println("BIG DEC");
//  System.out.println(hGrads2[i]);
//  System.out.println("Double");
//  System.out.println(hGrads2[i]);

//  }
}




void step3(double eta, double alpha){
 // 3. update input to hidden weights (gradients must be computed right-to-left but weights can be updated in any order
      for (int i = 0; i < ihWeights.length; ++i) // 0..2 (3)
      {
        for (int j = 0; j < ihWeights[0].length; ++j) // 0..3 (4)
        {
        	double delta2=eta*hGrads[j]*inputs[i];
        	ihWeights[i][j] +=delta2;//ihWeights[i][j].add( new BigDecimal(delta2)); // update
          //BigDecimal delta = eta.multiply(hGrads[j]).multiply(inputs[i]); // compute the new delta
          //ihWeights[i][j] =ihWeights[i][j].add( delta); // update
          //ihWeights[i][j] =ihWeights[i][j].add( alpha.multiply(ihPrevWeightsDelta[i][j])); // add momentum using previous delta. on first pass old value will be 0.0 but that's OK.
        	ihWeights[i][j]+=alpha*ihPrevWeightsDelta[i][j];
        }
      }

      // 3b. update input to hidden biases
            for (int i = 0; i < ihBiases.length; ++i)
            {
              //BigDecimal delta = eta.multiply(hGrads[i]).multiply(new BigDecimal(1.0)); // the 1.0 is the constant input for any bias; could leave out
              //ihBiases[i] = ihBiases[i].add(delta);
              //ihBiases[i] = ihBiases[i].add( alpha.multiply(ihPrevBiasesDelta[i]));
            	double delta = eta*hGrads[i]*1.0;
            	ihBiases[i]+=delta;
            	ihBiases[i]+=alpha*ihPrevBiasesDelta[i];
            }
}



public void step4(double eta, double alpha){


// 4. update hidden to output weights
      for (int i = 0; i < hoWeights.length; ++i)  // 0..3 (4)
      {
        for (int j = 0; j < hoWeights[0].length; ++j) // 0..1 (2)
        {
       //   BigDecimal delta = eta.multiply(oGrads[j]).multiply(ihOutputs[i]);  // see above: ihOutputs are inputs to next layer
        //  hoWeights[i][j] = hoWeights[i][j].add(delta);
         // hoWeights[i][j] = hoWeights[i][j].add( alpha.multiply(hoPrevWeightsDelta[i][j]));
         // hoPrevWeightsDelta[i][j] = delta;
        
        	//double delta = (new BigDecimal(eta).setScale(10, RoundingMode.HALF_UP).multiply(new BigDecimal(oGrads[j]).setScale(10, RoundingMode.HALF_UP)).multiply(ihOutputs[i]).setScale(10, RoundingMode.HALF_UP)).doubleValue();
        	double delta = eta*oGrads[j]*ihOutputs[i];
        	hoWeights[i][j] += delta;
        	hoWeights[i][j] += alpha * hoPrevWeightsDelta[i][j];
        	hoPrevWeightsDelta[i][j] = delta;
        
        }
      }

      // 4b. update hidden to output biases
      for (int i = 0; i < hoBiases.length; ++i)
      {
      //  BigDecimal delta = eta.multiply(oGrads[i]).multiply(new BigDecimal(1.0));
      //  hoBiases[i] = hoBiases[i].add(delta);
      //  hoBiases[i] = hoBiases[i].add(alpha.multiply(hoPrevBiasesDelta[i]));
      //  hoPrevBiasesDelta[i] = delta;
     
    	  double delta = eta * oGrads[i];
    	  hoBiases[i] += delta;
    	  hoBiases[i] += alpha * hoPrevBiasesDelta[i];
    	  hoPrevBiasesDelta[i] = delta;
      
      }
}

    public void UpdateWeights(double[] tValues, double eta, double alpha) throws Exception // update the weights and biases using back-propagation, with target values, eta (learning rate), alpha (momentum)
    {
      // assumes that SetWeights and ComputeOutputs have been called and so all the internal arrays and matrices have values (other than 0.0)
      if (tValues.length != numOutput)
        throw new Exception("target values not same length as output in UpdateWeights");

/*      double[] oGrads2 = new double[oGrads.length];
      // 1. compute output gradients
      for (int i = 0; i < oGrads.length; ++i)
      {
        BigDecimal derivative =(new BigDecimal(1).subtract(ihOutputs[i])).multiply(ihOutputs[i]); //(1 - outputs[i]) * (1 + outputs[i]); // derivative of tanh
        oGrads[i] = derivative.multiply((tValues[i].subtract(outputs[i])));
        
        //double derivative2 = ((1-ihOutputs[i].doubleValue())*ihOutputs[i].doubleValue());
        oGrads2[i]=derivative.doubleValue()*(tValues[i].doubleValue()-(outputs[i].doubleValue()));
        
      //  System.out.println("BIG DEC");
       // System.out.println(oGrads[i]);
        //System.out.println("Double");
        //System.out.println(oGrads2[i]);
        
      }*/
      
      step1(tValues);

      // 2. compute hidden gradients
      
/*      double[] hGrads2 = new double[hGrads.length];
      for (int i = 0; i < hGrads.length; ++i)
      {
        BigDecimal derivative = (new BigDecimal(1).subtract(ihOutputs[i])).multiply(ihOutputs[i]); // (1 / 1 + exp(-x))'  -- using output value of neuron
        BigDecimal sum = new BigDecimal(0.0);
        double sum2=0.0;
        for (int j = 0; j < numOutput; ++j){ // each hidden delta is the sum of numOutput terms
          sum =sum.add( oGrads[j].multiply(hoWeights[i][j])); // each downstream gradient * outgoing weight
          sum2=sum2+(oGrads[j].doubleValue()*(hoWeights[i][j].doubleValue()));
          
//        System.out.println("BIG DEC");
//        System.out.println(sum);
//        System.out.println("Double");
//        System.out.println(sum2);
//         
          
        }
        hGrads[i] = derivative.multiply(sum);
        hGrads2[i] = derivative.doubleValue()*sum2;
//      System.out.println("BIG DEC");
//      System.out.println(hGrads2[i]);
//      System.out.println("Double");
//      System.out.println(hGrads2[i]);

      }*/

      step2();

      // 3. update input to hidden weights (gradients must be computed right-to-left but weights can be updated in any order
    /*  for (int i = 0; i < ihWeights.length; ++i) // 0..2 (3)
      {
        for (int j = 0; j < ihWeights[0].length; ++j) // 0..3 (4)
        {
          BigDecimal delta = eta.multiply(hGrads[j]).multiply(inputs[i]); // compute the new delta        
          ihWeights[i][j] =ihWeights[i][j].add( delta); // update
          ihWeights[i][j] =ihWeights[i][j].add( alpha.multiply(ihPrevWeightsDelta[i][j])); // add momentum using previous delta. on first pass old value will be 0.0 but that's OK. 
        }
      }

      // 3b. update input to hidden biases
      for (int i = 0; i < ihBiases.length; ++i)
      {
        BigDecimal delta = eta.multiply(hGrads[i]).multiply(new BigDecimal(1.0)); // the 1.0 is the constant input for any bias; could leave out
        ihBiases[i] = ihBiases[i].subtract(delta);
        ihBiases[i] = ihBiases[i].add( alpha.multiply(ihPrevBiasesDelta[i]));
      }*/

     /* 	for (int i = 200; i < 220; i++) {
      		
			System.out.println(ihBiases[i]);
		}
      
      	System.out.println("-------------------");
      	*/
      step3(eta, alpha);
    /*  for (int i = 200; i < 220; i++) {
		System.out.println(ihBiases[i]);
	}
    */  
      
      // 4. update hidden to output weights
   /*   for (int i = 0; i < hoWeights.length; ++i)  // 0..3 (4)
      {
        for (int j = 0; j < hoWeights[0].length; ++j) // 0..1 (2)
        {
          BigDecimal delta = eta.multiply(oGrads[j]).multiply(ihOutputs[i]);  // see above: ihOutputs are inputs to next layer
          hoWeights[i][j] = hoWeights[i][j].add(delta);
          hoWeights[i][j] = hoWeights[i][j].add( alpha.multiply(hoPrevWeightsDelta[i][j]));
          hoPrevWeightsDelta[i][j] = delta;
        }
      }

      // 4b. update hidden to output biases
      for (int i = 0; i < hoBiases.length; ++i)
      {
        BigDecimal delta = eta.multiply(oGrads[i]).multiply(new BigDecimal(1.0));
        hoBiases[i] = hoBiases[i].add(delta);
        hoBiases[i] = hoBiases[i].add(alpha.multiply(hoPrevBiasesDelta[i]));
        hoPrevBiasesDelta[i] = delta;
      }*/
      
      step4(eta, alpha);

    }

    public void SetWeights(double[] weights)
    {
      // copy weights and biases in weights[] array to i-h weights, i-h biases, h-o weights, h-o biases
      int numWeights = (numInput * numHidden) + (numHidden * numOutput) + numHidden + numOutput;
      if (weights.length != numWeights)
    	  System.out.println("Err");
    	  //       throw new Exception("The weights array length: " + weights.length + " does not match the total number of weights and biases: " + numWeights);

      int k = 0; // points into weights param

      for (int i = 0; i < numInput; ++i)
        for (int j = 0; j < numHidden; ++j)
          ihWeights[i][j] = weights[k++];

      for (int i = 0; i < numHidden; ++i)
        ihBiases[i] = weights[k++];

      for (int i = 0; i < numHidden; ++i)
        for (int j = 0; j < numOutput; ++j)
          hoWeights[i][j] = weights[k++];

      for (int i = 0; i < numOutput; ++i)
        hoBiases[i] = weights[k++];
    }

    public double[] GetWeights()
    {
      int numWeights = (numInput * numHidden) + (numHidden * numOutput) + numHidden + numOutput;
      double[] result = new double[numWeights];
      int k = 0;
      for (int i = 0; i < ihWeights.length; ++i)
        for (int j = 0; j < ihWeights[0].length; ++j)
          result[k++] = ihWeights[i][j];
      for (int i = 0; i < ihBiases.length; ++i)
        result[k++] = ihBiases[i];
      for (int i = 0; i < hoWeights.length; ++i)
        for (int j = 0; j < hoWeights[0].length; ++j)
          result[k++] = hoWeights[i][j];
      for (int i = 0; i < hoBiases.length; ++i)
        result[k++] = hoBiases[i];
      return result;
    }

    public double[] ComputeOutputs(double[] xValues)
    {
      if (xValues.length != numInput)
       System.out.println("ERR");
    	  //throw new Exception("Inputs array length " + inputs.length + " does not match NN numInput value " + numInput);

      for (int i = 0; i < numHidden; ++i)
        ihSums[i] = 0.0;
      for (int i = 0; i < numOutput; ++i)
        hoSums[i] = 0.0;

      for (int i = 0; i < xValues.length; ++i) // copy x-values to inputs
        this.inputs[i] = xValues[i];
      
      for (int j = 0; j < numHidden; ++j)  // compute input-to-hidden weighted sums
        for (int i = 0; i < numInput; ++i)
          ihSums[j] += this.inputs[i] * ihWeights[i][j];

      for (int i = 0; i < numHidden; ++i)  // add biases to input-to-hidden sums
        ihSums[i] += ihBiases[i];

      for (int i = 0; i < numHidden; ++i)   // determine input-to-hidden output
        ihOutputs[i] = SigmoidFunction(ihSums[i]);

      for (int j = 0; j < numOutput; ++j)   // compute hidden-to-output weighted sums
        for (int i = 0; i < numHidden; ++i)
          hoSums[j] += ihOutputs[i] * hoWeights[i][j];

      for (int i = 0; i < numOutput; ++i)  // add biases to input-to-hidden sums
        hoSums[i] += hoBiases[i];

      for (int i = 0; i < numOutput; ++i)   // determine hidden-to-output result
        this.outputs[i] = SoftmaxFunction(new BigDecimal(hoSums[i]), hoSums);//HyperTanFunction(hoSums[i]);
      	
      double[] result = new double[numOutput]; // could define a GetOutputs method instead
     

      return outputs;
    } // ComputeOutputs

    private static double StepFunction(double x) // an activation function that isn't compatible with back-propagation bcause it isn't differentiable
    {
      if (x > 0.0) return 1.0;
      else return 0.0;
    }

    private static double SigmoidFunction(double x)
    {
      if (x < -45.0) return 0.0;
      else if (x > 45.0) return 1.0;
      else return 1.0 / (1.0 + Math.exp(-x));
    }

    private static double HyperTanFunction(double x)
    {
      if (x < -10.0) return -1.0;
      else if (x > 10.0) return 1.0;
      else return Math.tanh(x);
    }
    
    private double SoftmaxFunction(BigDecimal hoSums2, double[] hoSums3) {

		//BigDecimal sum= new BigDecimal(0);
		//for (int i = 0; i < hoSums3.length; i++) {
		//	sum=sum.add(BigDecimalMath.pow(new BigDecimal(Math.E).setScale(100, BigDecimal.ROUND_FLOOR),hoSums3[i].setScale(100, BigDecimal.ROUND_FLOOR)));
		//}
		double sum2 =0.0;
		for(int i =0; i<hoSums3.length;i++){
			sum2 = sum2 + (Math.pow(Math.E, hoSums3[i]));
		}
		/// sys sum stava
		

		
		//System.out.println("===========sum==================");
		//System.out.println(sum);
		//BigDecimal b =BigDecimalMath.pow(new BigDecimal(Math.E).setScale(100, BigDecimal.ROUND_FLOOR), (hoSums2.setScale(100, BigDecimal.ROUND_FLOOR)));
		//System.out.println("==========b==================");
		double bb=Math.pow(Math.E, hoSums2.doubleValue());
		
		//BigDecimal sum = new BigDecimal(sum2);
		//BigDecimal b = new BigDecimal(bb);
		
		// i za B vaji 
		
		//System.out.println(b);
		//BigDecimal a  =b.divide(sum,MathContext.DECIMAL128);
		
		double a=bb/sum2;
		//System.out.println("AZ SUM REZULTATA NA SOFTMAX FUNCTION");
		//System.out.println(a);
//		double bbb=bb/(sum2);
//		System.out.println("double");
//		System.out.println(bbb);
//		//double gosho = sum.doubleValue();
//		System.out.println("GOSHO:  ");
//		System.out.println(a);
//		
		
		return a;	
	//}
}
    
    
  } // class NeuralNetwork

  // ===========================================================================
